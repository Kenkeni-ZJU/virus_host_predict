{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "lib_dir = '../mylibs'\n",
    "if lib_dir not in sys.path:\n",
    "    sys.path.append(lib_dir)\n",
    "    \n",
    "import vhdb as vhdb\n",
    "import new_features_predict as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filein = '../inputs/VHDB_25_1_2019.p'\n",
    "with open(filein, 'rb') as f:\n",
    "    V_H = pickle.load( f)\n",
    "hosts = V_H.hosts\n",
    "viruses = V_H.viruses\n",
    "subspecies =V_H.subspecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/holdout_Bact_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input file\n",
    "#subsets = [(label,taxlevel,pool,taxlevelpool),...]\n",
    "subsetfile = '../inputs/holdout_Bact.csv'\n",
    "baltimore = 'all'\n",
    "\n",
    "# Everthing in the same order as the features list\n",
    "\n",
    "features = ['DNA','AA','PC','Domains']\n",
    "# filepath for the fasta, faa and domain files\n",
    "filepaths = [\n",
    "                '/home4/youn01f/Desktop/workspace/newData/fasta',\n",
    "                '/home4/youn01f/Desktop/workspace/newData/faa','/home4/youn01f/Desktop/workspace/newData/faa',\n",
    "                '/home4/youn01f/Desktop/workspace/newData/pfs']\n",
    "file_exts = ['fasta','fasta','fasta','pfs']#,'fasta',fasta]\n",
    "\n",
    "# A list of kmers for each feature set to be tested\n",
    "kmer_lists = [\n",
    "              [2,6,9], # dna \n",
    "              [3,4], # aa\n",
    "              [5,6] ,    #pc\n",
    "              [0]] #domains\n",
    "\n",
    "\n",
    "# symbol dictionaries \n",
    "na_dict = {'mod':4,'a':0,'c':1,'g':2,'t':3}\n",
    "aa_dict = {'mod':20 ,'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,\n",
    "              'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19}\n",
    "pc_dict = {'mod':7, 'C':0,\n",
    "            'A':1,'G':1,'V':1,\n",
    "           'I':2,'L':2,'F':2,'P':2,\n",
    "           'M':3,'S':3,'T':3,'Y':3,\n",
    "           'H':4,'N':4,'Q':4,'W':4,\n",
    "           'R':5,'K':5,\n",
    "           'D':6,'E':6}\n",
    "symbol_dicts = [\n",
    "                na_dict,aa_dict,pc_dict, \n",
    "                {}]\n",
    "\n",
    "classifiers = ['SVM_lin']\n",
    "\n",
    "# Output file for the results\n",
    "out_file = Path(subsetfile).stem \n",
    "results_file =  f'../results/{out_file}_results.csv'\n",
    "model = 1\n",
    "print (results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bacteroidetes', 'phylum', 'Bacteria', 'kingdom', 'DNA', 'Siphoviridae'),\n",
       " ('Actinobacteria', 'class', 'Bacteria', 'kingdom', 'DNA', 'Siphoviridae'),\n",
       " ('Firmicutes', 'phylum', 'Bacteria', 'kingdom', 'DNA', 'Siphoviridae')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_info = pd.read_csv(subsetfile)\n",
    "subsets = label_info.apply(tuple, axis=1).tolist()\n",
    "(subsets[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter get_class_lists for holdout classifier. \n",
    "Training data - everything but holdout viruse holdout virus group\n",
    "Test data - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_lists_ho(subset,viruses,hosts):\n",
    "    \n",
    "    (label,label_tax,pool,pool_tax,balt,v_holdout) = subset\n",
    "    data_lists = {'training':[[],[]],'test':[[],[]]}\n",
    "    # Get a list of all the viruses in the labelled class and the rest of the pool\n",
    "    baltimore = 'NA'  # all slasses (except satallites)\n",
    "    for v, vd in viruses.items() :\n",
    "        if  baltimore in vd['class']: \n",
    "            \n",
    "            if  vd['family']!= v_holdout:\n",
    "                ds_name ='training'\n",
    "            else:\n",
    "                ds_name = 'test'\n",
    "            \n",
    "            host_labels = [hosts[h][label_tax] for h in vd['hosts'] if hosts[h][pool_tax] == pool]    \n",
    "            if len(host_labels) > 0:\n",
    "                if label in host_labels:\n",
    "                    data_lists[ds_name][0].append ((v,label))\n",
    "                else:\n",
    "                    data_lists[ds_name][1].append ((v,'Other'))    \n",
    "                                                    \n",
    "    \n",
    " # Keep all the viruses in the holdout set as test data. they will not be same size   \n",
    "    datasets = {'training':{},'test':{}}\n",
    "    nmax = {'training':400,'test':50}\n",
    "    for k,data_list in data_lists.items():\n",
    "        datas =[]\n",
    "        n = min (len(data_list[0]),len(data_list[1]),nmax[k])\n",
    "        # randomly choose n viruses from both classes\n",
    "        for clss in data_list: \n",
    "            data = (random.sample(clss, n))\n",
    "            datas.extend(data)\n",
    "            \n",
    "       # shuffle together\n",
    "        random.shuffle(datas)\n",
    "        \n",
    "        for v,l in datas:\n",
    "            datasets [k][v]= {'label':l, 'refseqs':viruses[v]['refseqs']}\n",
    "    ntrain = (len(datasets['training']))\n",
    "    ntest = (len(datasets['test']))\n",
    "    print(f' ho training v {ntrain} , test v {ntest}')\n",
    "    return datasets,ntrain,ntest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_lists(subset,viruses,hosts):\n",
    "# returns two list of viruses one for each  positive class and negative class    \n",
    "    (label,label_tax,pool,pool_tax,baltimore,v_ho) = subset\n",
    "    \n",
    "    if baltimore =='all':\n",
    "        baltimore = 'NA'  # all classes  contain either 'DNA' or 'RNA'\n",
    "    \n",
    "    pos_neg =  [[],[]]\n",
    "    # Get 2 lists of all the viruses in the labelled class and the rest of the pool\n",
    "    for v, vd in viruses.items() :\n",
    "        if baltimore in vd['class']:\n",
    "            \n",
    "            host_labels = [hosts[h][label_tax] for h in vd['hosts'] if hosts[h][pool_tax] == pool]\n",
    "            \n",
    "            if len(host_labels) > 0:\n",
    "                if label in host_labels:\n",
    "                    pos_neg[0].append ((v,label))\n",
    "                else:\n",
    "                    pos_neg[1].append((v,'Other'))    \n",
    "                         \n",
    "    # Get a random sample for each class of size n , the size of smallest class \n",
    "    #restrict to 400\n",
    "    n = min(len(pos_neg[0]),len(pos_neg[1]),400)\n",
    "   # n=6 # test \n",
    "    datas =  []\n",
    "    random.seed(10)\n",
    "    for clss in pos_neg:\n",
    "        data = (random.sample(clss, n))\n",
    "        datas.append(data)\n",
    "    \n",
    "    #split into training and test sets     \n",
    "    split = 0.75 \n",
    "    trn_tst_lst =  [[],[]]  #2 lists for training and test viruses\n",
    "    \n",
    "    for viruslist in datas:\n",
    "        for i in range(n):\n",
    "            if random.random() < split:\n",
    "                trn_tst_lst[0].append(viruslist[i]) \n",
    "            else:\n",
    "                trn_tst_lst[1].append(viruslist[i]) \n",
    "  # Convert to dataset dictionaries\n",
    "    datadicts = {'training':{},'test':{}}\n",
    "    ds = datadicts.keys()\n",
    "    for i,lst in enumerate(trn_tst_lst):\n",
    "        random.shuffle(lst)\n",
    "        k = list(ds)[i]\n",
    "        for v,l in lst:\n",
    "            datadicts [k][v]= {'label':l, 'refseqs':viruses[v]['refseqs']}\n",
    "        ntrain  =len(datadicts['training'])\n",
    "        ntest = len(datadicts['test'])\n",
    "    \n",
    "    return (datadicts,ntrain,ntest)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =0\n",
    "def test_dataset(subset,datasets,n): \n",
    "    (label,label_tax,pool,pool_tax,balt,v_holdout,t_g) = subset\n",
    "    global model\n",
    "    print(model, features,kmer_lists)\n",
    "    for feature in features:\n",
    "        index = features.index(feature)\n",
    "        kmers = kmer_lists[index]\n",
    "        filepath = filepaths[index]\n",
    "        symbol_dict = symbol_dicts[index]\n",
    "        ext = file_exts[index]\n",
    "        print (index,kmers,filepath,symbol_dict)\n",
    "\n",
    "        if features != 'PC': #PC same sequence as for AA\n",
    "            filepath = filepaths[index]\n",
    "            print ( f'getting {feature} sequences from {filepath}')\n",
    "            sequences = fp.get_sequences(filepath,datasets,ext)\n",
    "            \n",
    "        for k in kmers:\n",
    "            model += 1\n",
    "        # get feature matrix , test prediction and write results to CSV file\n",
    "            x_train,x_test,y_train,y_test = fp.get_feature_matrices(sequences,datasets,label,k,symbol_dict)\n",
    "            print (np.shape(x_train), np.shape(x_test),np.shape(y_train),np.shape(y_test))\n",
    "            #print (type(x_train), type(x_test),type(y_train),type(y_test))\n",
    "            results = fp.test_prediction(x_train,x_test,y_train,y_test)\n",
    "            results.update ({'N in class': n, 'Features':feature, 'k':k})\n",
    "            print(results)\n",
    "            results2CSV (results,subset, results_file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results2CSV(results,subset, csvfile):\n",
    "    (label,label_tax,pool,pool_tax,balt,v_holdout,t_g) = subset\n",
    "    #results = { k : round(v, 3) for k,v in results.items()}\n",
    "    results['positive label'] = label\n",
    "    results['label tax group']= label_tax\n",
    "    results['pool label']= pool\n",
    "    results['pool tax group']= pool_tax\n",
    "    results['Baltimore'] = balt\n",
    "    results['virus holdout group'] = v_holdout\n",
    "    results['training group']= t_g\n",
    "    fieldnames = ['positive label','label tax group','pool label','pool tax group',\n",
    "                          'Baltimore','virus holdout group','training group', 'N in class' , \n",
    "                          'Features','k','AUC' ]\n",
    "    if os.path.isfile(csvfile):\n",
    "        with open(csvfile, 'a') as csvfile:\n",
    "            #fieldnames = ['positive label','label tax group','pool label','pool tax group','Baltimore','virus holdout group','training group', 'N in class' , 'Features','k','AUC','accuracy', 'specificity','sensitivity', 'TN/FP/FN/TP' ]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerow(results) \n",
    "    else:\n",
    "        with open(csvfile, 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerow(results)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets2csv(datasets,tg):\n",
    "#test_dataset((*subset,'Holdout'),datasets_ho,n1)\n",
    "    with open('datasets.csv', 'a') as csvfile:\n",
    "        fields =['train/test','ho/all','virus', 'label', 'refseqs']\n",
    "        writer = csv.DictWriter(csvfile,fields)\n",
    "        writer.writeheader()\n",
    "        for k,d in (datasets_ho.items()):\n",
    "            for v,d2 in d.items():\n",
    "                row = {'train/test': k ,'ho/all':tg, 'virus':viruses[v]['family']}\n",
    "                row.update(d2)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get AUC for each feature set for each data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Actinobacteria', 'class', 'Bacteria', 'kingdom', 'DNA', 'Siphoviridae')\n",
      " ho training v 68 , test v 100\n",
      "16 ['DNA', 'AA', 'PC', 'Domains'] [[2, 6, 9], [3, 4], [5, 6], [0]]\n",
      "0 [2, 6, 9] /home4/youn01f/Desktop/workspace/newData/fasta {'mod': 4, 'a': 0, 'c': 1, 'g': 2, 't': 3}\n",
      "getting DNA sequences from /home4/youn01f/Desktop/workspace/newData/fasta\n",
      "viruses number of missing files 0\n",
      "X_training:Extacting  features  of length  2  from   68 sequences\n",
      "(68, 16)\n",
      "X_test: Extacting  features  of length  2  from   100 sequences\n",
      "(100, 16)\n",
      "(68, 16) (100, 16) (68,) (100,)\n",
      "{'AUC': 0.989, 'N in class': 68, 'Features': 'DNA', 'k': 2}\n",
      "X_training:Extacting  features  of length  6  from   68 sequences\n",
      "(68, 4096)\n",
      "X_test: Extacting  features  of length  6  from   100 sequences\n",
      "(100, 4096)\n",
      "(68, 4096) (100, 4096) (68,) (100,)\n",
      "{'AUC': 0.974, 'N in class': 68, 'Features': 'DNA', 'k': 6}\n",
      "X_training:Extacting  features  of length  9  from   68 sequences\n",
      "(68, 262144)\n",
      "X_test: Extacting  features  of length  9  from   100 sequences\n",
      "(100, 262144)\n",
      "(68, 262144) (100, 262144) (68,) (100,)\n",
      "{'AUC': 0.971, 'N in class': 68, 'Features': 'DNA', 'k': 9}\n",
      "1 [3, 4] /home4/youn01f/Desktop/workspace/newData/faa {'mod': 20, 'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}\n",
      "getting AA sequences from /home4/youn01f/Desktop/workspace/newData/faa\n",
      "viruses number of missing files 0\n",
      "X_training:Extacting  features  of length  3  from   68 sequences\n",
      "(68, 8000)\n",
      "X_test: Extacting  features  of length  3  from   100 sequences\n",
      "(100, 8000)\n",
      "(68, 8000) (100, 8000) (68,) (100,)\n",
      "{'AUC': 0.975, 'N in class': 68, 'Features': 'AA', 'k': 3}\n",
      "X_training:Extacting  features  of length  4  from   68 sequences\n",
      "(68, 160000)\n",
      "X_test: Extacting  features  of length  4  from   100 sequences\n",
      "(100, 160000)\n",
      "(68, 160000) (100, 160000) (68,) (100,)\n",
      "{'AUC': 0.972, 'N in class': 68, 'Features': 'AA', 'k': 4}\n",
      "2 [5, 6] /home4/youn01f/Desktop/workspace/newData/faa {'mod': 7, 'C': 0, 'A': 1, 'G': 1, 'V': 1, 'I': 2, 'L': 2, 'F': 2, 'P': 2, 'M': 3, 'S': 3, 'T': 3, 'Y': 3, 'H': 4, 'N': 4, 'Q': 4, 'W': 4, 'R': 5, 'K': 5, 'D': 6, 'E': 6}\n",
      "getting PC sequences from /home4/youn01f/Desktop/workspace/newData/faa\n",
      "viruses number of missing files 0\n",
      "X_training:Extacting  features  of length  5  from   68 sequences\n",
      "(68, 16807)\n",
      "X_test: Extacting  features  of length  5  from   100 sequences\n",
      "(100, 16807)\n",
      "(68, 16807) (100, 16807) (68,) (100,)\n",
      "{'AUC': 0.95, 'N in class': 68, 'Features': 'PC', 'k': 5}\n",
      "X_training:Extacting  features  of length  6  from   68 sequences\n",
      "(68, 117649)\n",
      "X_test: Extacting  features  of length  6  from   100 sequences\n",
      "(100, 117649)\n",
      "(68, 117649) (100, 117649) (68,) (100,)\n",
      "{'AUC': 0.931, 'N in class': 68, 'Features': 'PC', 'k': 6}\n",
      "3 [0] /home4/youn01f/Desktop/workspace/newData/pfs {}\n",
      "getting Domains sequences from /home4/youn01f/Desktop/workspace/newData/pfs\n",
      "viruses number of missing files 1\n",
      "X_training:Extacting  features  of length  0  from   67 sequences\n",
      "(67, 906)\n",
      "X_test: Extacting  features  of length  0  from   100 sequences\n",
      "(100, 906)\n",
      "(67, 906) (100, 906) (67,) (100,)\n",
      "{'AUC': 0.883, 'N in class': 68, 'Features': 'Domains', 'k': 0}\n",
      "24 ['DNA', 'AA', 'PC', 'Domains'] [[2, 6, 9], [3, 4], [5, 6], [0]]\n",
      "0 [2, 6, 9] /home4/youn01f/Desktop/workspace/newData/fasta {'mod': 4, 'a': 0, 'c': 1, 'g': 2, 't': 3}\n",
      "getting DNA sequences from /home4/youn01f/Desktop/workspace/newData/fasta\n",
      "viruses number of missing files 0\n",
      "X_training:Extacting  features  of length  2  from   605 sequences\n",
      "(605, 16)\n",
      "X_test: Extacting  features  of length  2  from   195 sequences\n",
      "(195, 16)\n",
      "(605, 16) (195, 16) (605,) (195,)\n",
      "{'AUC': 0.994, 'N in class': 605, 'Features': 'DNA', 'k': 2}\n",
      "X_training:Extacting  features  of length  6  from   605 sequences\n"
     ]
    }
   ],
   "source": [
    "for subset in subsets[1:]:\n",
    "    print(subset)\n",
    "#     # for holdout data  \n",
    "    datasets_ho,n1,n2 = get_class_lists_ho (subset,viruses,hosts)\n",
    "    #datasets2csv(datasets_ho,'ho')\n",
    "    test_dataset((*subset,'holdout'),datasets_ho,n1)\n",
    "    \n",
    "    #for all data\n",
    "    datasets_all,ntrain,n_ho = get_class_lists(subset,viruses,hosts)\n",
    "    #datasets2csv(datasets_all,'all')\n",
    "    test_dataset((*subset,'all'), datasets_all,ntrain)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get numbers for each data set in subsets. \n",
    "If too small remove"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
