{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import timeit\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "lib_dir = '../mylibs'\n",
    "if lib_dir not in sys.path:\n",
    "    sys.path.append(lib_dir)\n",
    "    \n",
    "import vhdb as vhdb\n",
    "import features_predict as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in Bacteria_DNA_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input file\n",
    "#subsets = [(label,taxlevel,pool,taxlevelpool),...]\n",
    "subsetfile = '../inputs/Bacteria_DNA.csv'\n",
    "baltimore = 'all'\n",
    "vhdbfile = '../inputs/VHDB_25_1_2019.p'\n",
    "\n",
    "\n",
    "# Everthing in the same order as the features list\n",
    "\n",
    "feature_list = ['DNA','AA','PC','Domains']\n",
    "# filepath for the fasta, faa and domain files\n",
    "filepaths = [\n",
    "                '/home4/youn01f/Desktop/workspace/newData/fasta',\n",
    "                '/home4/youn01f/Desktop/workspace/newData/faa','/home4/youn01f/Desktop/workspace/newData/faa',\n",
    "                '/home4/youn01f/Desktop/workspace/newData/pfs']\n",
    "file_exts = ['fasta','fasta','fasta','pfs']#,'fasta',fasta]\n",
    "\n",
    "# A list of kmers for each feature set to be tested\n",
    "kmer_lists = [[1, 2,3,4,5,6,7,8,9], # dna \n",
    "              [1, 2, 3,4], # aa\n",
    "              [1, 2,3,4,5,6] ,    #pc\n",
    "             [0]] #domains\n",
    "\n",
    "# symbol dictionaries \n",
    "na_dict = {'mod':4,'a':0,'c':1,'g':2,'t':3}\n",
    "aa_dict = {'mod':20 ,'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,\n",
    "              'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19}\n",
    "pc_dict = {'mod':7, 'C':0,\n",
    "            'A':1,'G':1,'V':1,\n",
    "           'I':2,'L':2,'F':2,'P':2,\n",
    "           'M':3,'S':3,'T':3,'Y':3,\n",
    "           'H':4,'N':4,'Q':4,'W':4,\n",
    "           'R':5,'K':5,\n",
    "           'D':6,'E':6}\n",
    "symbol_dicts = [ na_dict,aa_dict,pc_dict, {}]\n",
    "\n",
    "classifiers = ['SVM_lin']\n",
    "\n",
    "# Output file for the results\n",
    "\n",
    "results_file = f'results/{Path(subsetfile).stem}_results.csv'\n",
    "print( f'Results in {results_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filein, 'rb') as f:\n",
    "    V_H = pickle.load( f)\n",
    "hosts = V_H.hosts\n",
    "viruses = V_H.viruses\n",
    "subspecies =V_H.subspecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_info = pd.read_csv(subsetfile)\n",
    "subsets = label_info.apply(tuple, axis=1).tolist()\n",
    "len(subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " size of training 44 and test set   16\n",
      " size of training 158 and test set   36\n",
      " size of training 178 and test set   44\n",
      " size of training 395 and test set   95\n",
      " size of training 402 and test set   98\n",
      " size of training 42 and test set   14\n",
      " size of training 53 and test set   17\n",
      " size of training 402 and test set   98\n",
      " size of training 42 and test set   14\n",
      " size of training 402 and test set   98\n",
      " size of training 44 and test set   16\n",
      " size of training 48 and test set   16\n",
      " size of training 280 and test set   72\n",
      " size of training 335 and test set   79\n",
      " size of training 82 and test set   18\n",
      " size of training 160 and test set   38\n",
      " size of training 161 and test set   39\n",
      " size of training 402 and test set   98\n",
      " size of training 58 and test set   18\n",
      " size of training 105 and test set   21\n",
      " size of training 124 and test set   32\n",
      " size of training 126 and test set   32\n",
      " size of training 54 and test set   18\n",
      " size of training 116 and test set   30\n",
      " size of training 402 and test set   98\n",
      " size of training 62 and test set   18\n",
      " size of training 50 and test set   16\n",
      " size of training 197 and test set   51\n",
      " size of training 206 and test set   54\n",
      " size of training 46 and test set   16\n",
      " size of training 50 and test set   16\n",
      " size of training 54 and test set   18\n",
      " size of training 175 and test set   41\n",
      " size of training 210 and test set   56\n",
      " size of training 402 and test set   98\n",
      " size of training 67 and test set   19\n",
      " size of training 68 and test set   20\n",
      " size of training 120 and test set   30\n",
      " size of training 156 and test set   34\n",
      " size of training 169 and test set   39\n",
      " size of training 286 and test set   72\n",
      " size of training 43 and test set   15\n",
      " size of training 52 and test set   16\n",
      " size of training 402 and test set   98\n",
      " size of training 402 and test set   98\n",
      " size of training 65 and test set   19\n",
      " size of training 70 and test set   20\n",
      " size of training 72 and test set   20\n",
      " size of training 402 and test set   98\n",
      " size of training 402 and test set   98\n",
      " size of training 402 and test set   98\n",
      " size of training 402 and test set   98\n",
      " size of training 402 and test set   98\n",
      " size of training 91 and test set   19\n",
      " size of training 110 and test set   28\n",
      " size of training 402 and test set   98\n",
      " size of training 101 and test set   21\n",
      " size of training 128 and test set   32\n",
      " size of training 133 and test set   33\n",
      " size of training 402 and test set   98\n",
      " size of training 42 and test set   14\n",
      " size of training 44 and test set   16\n",
      " size of training 158 and test set   36\n",
      " size of training 169 and test set   39\n",
      " size of training 42 and test set   14\n",
      " size of training 64 and test set   18\n",
      " size of training 74 and test set   20\n"
     ]
    }
   ],
   "source": [
    "for subset in subsets:\n",
    "    class_data,n = fp.get_class_lists(subset,viruses,hosts)\n",
    "    datasets = fp.split_data(viruses,class_data,n)\n",
    "    (label,label_tax,pool,pool_tax,baltimore) = subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(500,250,251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_kmers (sequences,k,symbol_dict):\n",
    "    if k == 0: # domainsdom_list = list({dom for s in sequence.values() for dom in s})\n",
    "        \n",
    "        X,seq_index,f_index = get_domains(sequences,symbol_dict)\n",
    "    else:  # everything else\n",
    "        vocab_len =  symbol_dict['mod'] **k\n",
    "        f_index = get_feature_names(symbol_dict,k)\n",
    "        seq_index = []\n",
    "        X = np.zeros((len(sequences) ,vocab_len))\n",
    "\n",
    "        for i,(accn, seq) in enumerate (sequences.items()):\n",
    "            X[i] = get_word_frequ (seq,k, vocab_len,symbol_dict)\n",
    "            seq_index.append(accn)\n",
    "# normalise for length of genome(s)\n",
    "    XN = np.divide(X,X.sum(axis = 1)[:,None])\n",
    "    print (np.shape(XN)) \n",
    "    return XN,seq_index,f_index\n",
    "\n",
    "def get_domains(sequence,dom_list):\n",
    "    ''' input: sequences dictionary of training and test sequence dicts'''\n",
    "    seq_index = []\n",
    "    X = np.zeros((len(sequence) ,len(dom_list)))\n",
    "    for i,(accn, seq) in enumerate (sequence.items()):\n",
    "        for dom in seq:\n",
    "            j = dom_list.index(dom)\n",
    "            X[i,j] += 1\n",
    "        seq_index.append(accn)\n",
    "    return X,seq_index,dom_list\n",
    "\n",
    "def get_word_frequ(sequence,k,vocab_len,symbol_dict):\n",
    "    \"\"\"returns the word frequency array for all genomes for a virus \"\"\"\n",
    "    word_freqs = np.zeros(shape =(vocab_len),dtype=np.int32)\n",
    "    for i in range(len(sequence)-k+1):\n",
    "        word = sequence[i:i+k]\n",
    "        if check_word(word,symbol_dict):\n",
    "            kmer_index = patternToNumber (word,k, symbol_dict)\n",
    "            word_freqs [ kmer_index] += 1\n",
    "            # If this for DNA/RNA then add the k-mers complinent\n",
    "#             if symbol_dict['mod'] == 4:\n",
    "#                 word_freqs [patternToNumber(compliment(word),k,symbol_dict)] +=1   \n",
    "    return word_freqs\n",
    "\n",
    "def get_sequences(faapath,datasets,ext):\n",
    "    missing_files = []\n",
    "    ext = f'.{ext}'\n",
    "    sequences = {'training':{},'test':{}}\n",
    "    for key,dataset in datasets.items():\n",
    "        for v,vdict in dataset.items():\n",
    "            sequence = defaultdict(str)\n",
    "            for ref in vdict['refseqs']:\n",
    "                filename = os.path.join(faapath, ref.strip()+ ext)\n",
    "                try:\n",
    "                    with open(filename, 'rt') as handle:\n",
    "                        if ext == '.fasta':\n",
    "                            for record in SeqIO.parse(handle, \"fasta\"):                   \n",
    "                                sequence[v] += str(record.seq)\n",
    "                        else: # pfs file - list of domains  \n",
    "                            line = handle.read()\n",
    "                            sequence[v] = [d for d in line.split()]\n",
    "                                \n",
    "                except IOError:\n",
    "                    missing_files.append(ref)\n",
    "            sequences[key].update(sequence)\n",
    "    print ( f'missing files {len(missing_files)}')\n",
    "    return (sequences)\n",
    "\n",
    "\n",
    "def symbolToNumber (symbol,symbols):\n",
    "    return symbols[symbol]\n",
    "\n",
    "def patternToNumber (pattern,k,symbol_dict):\n",
    "    mod = symbol_dict['mod']\n",
    "    number = 0\n",
    "    for i in range (0,k):\n",
    "        n = symbolToNumber ( pattern [k-1-i],symbol_dict)\n",
    "        number += n* mod**i \n",
    "    return number\n",
    "\n",
    "def get_feature_names(symbols,k):\n",
    "    all_kmers = []\n",
    "    n2sym = { v:k for k,v in symbols.items()}\n",
    "    mod = symbols['mod']\n",
    "    for number in range(mod**k):\n",
    "        pattern = ''\n",
    "        for i in range (0,k):\n",
    "            n = number% mod\n",
    "            number = (number - n)/mod \n",
    "            symbol = n2sym[n]\n",
    "            pattern = symbol + pattern\n",
    "        all_kmers.append (pattern)\n",
    "    return all_kmers\n",
    "\n",
    "def check_word(mystring,symbols):\n",
    "    \"\"\"Check for illegal characters \"\"\"\n",
    "    return all(c in symbols for c in mystring)\n",
    "\n",
    "def compliment(dna):\n",
    "    \"\"\"Return the reverse compliment of a sequence \"\"\"\n",
    "    bp = { 'a':'t', 't':'a', 'c':'g','g':'c'}\n",
    "    revdna =''    \n",
    "    for i in range(len(dna)-1,-1,-1):\n",
    "         revdna += bp[dna[i]]\n",
    "    return (revdna)\n",
    "\n",
    "def  get_labels(label,v_index,dataset):      \n",
    "    for i,v in enumerate(v_index):\n",
    "        y = [1 if dataset[v]['label'] ==  label else 0 for v in v_index ] \n",
    "    return np.asarray (y)\n",
    "\n",
    "def get_feature_matrices(sequences,datasets,k,symbol_dict):\n",
    "   \n",
    "    if k == 0: # domainsdom_list = list({dom for s in sequence.values() for dom in s})\n",
    "        symbol_dict = list({dom for sequence in sequences.values() for s in sequence.values() for dom in s})\n",
    "        \n",
    "    \n",
    "   \n",
    "   # print (f'X_training:Extacting  {feature}  of length  {k}  from   {len(sequences[\"training\"])} sequences')\n",
    "   # start_time = timeit.default_timer()\n",
    "    X_train, seq_index, f_index = extract_kmers (sequences['training'],k,symbol_dict)\n",
    "    Y_train =  get_labels (label,seq_index,datasets['training'])\n",
    "   # print ('timeit: ',timeit.default_timer() - start_time)\n",
    "   # print (f'X_test: Extacting  {feature}  of length  {k}  from   {len(sequences[\"test\"])} sequences')\n",
    "   # start_time = timeit.default_timer()\n",
    "    X_test, seq_index, f_index = extract_kmers (sequences['test'],k,symbol_dict)\n",
    "    Y_test =  get_labels (label,seq_index,datasets['test'])\n",
    "    #print ('timit: ', timeit.default_timer() - start_time)\n",
    "\n",
    "    return X_train,X_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['DNA','AA','PC','Domains']\n",
    "for subset in subsets:\n",
    "    class_data,n = fp.get_class_lists(subset,viruses,hosts)\n",
    "    datasets = fp.split_data(viruses,class_data,n)\n",
    "    (label,label_tax,pool,pool_tax,baltimore) = subset\n",
    "    print  (label,label_tax,pool,pool_tax,baltimore)\n",
    "   \n",
    "    for feature in features:\n",
    "        \n",
    "        index = feature_list.index(feature)\n",
    "        kmers = kmer_lists[index]\n",
    "        filepath = filepaths[index]\n",
    "        symbol_dict = symbol_dicts[index]\n",
    "        ext = file_exts[index]\n",
    "        print (index,kmers,filepath,symbol_dict)\n",
    "\n",
    "        if features != 'PC': #PC same sequence as for AA\n",
    "            filepath = filepaths[index]\n",
    "            print ( f'getting {feature} sequences from {filepath}')\n",
    "            sequences = get_sequences(filepath,datasets,ext)\n",
    "\n",
    "        for k in kmers: \n",
    "            x_train,x_test,y_train,y_test = get_feature_matrices(sequences,datasets,k,symbol_dict)\n",
    "            print (np.shape(x_train), np.shape(x_test),np.shape(y_train),np.shape(y_test))\n",
    "            #print (type(x_train), type(x_test),type(y_train),type(y_test))\n",
    "            results = fp.test_prediction(x_train,x_test,y_train,y_test)\n",
    "            results.update ({'N in class': n, 'Features':feature, 'k':k})\n",
    "            print(results)\n",
    "            fp.results2CSV (results,subset, results_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
